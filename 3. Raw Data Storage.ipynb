{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "057476f0-89e6-443d-81c4-3e8d7e780947",
   "metadata": {},
   "source": [
    "#### Raw Data Storage\n",
    "1. creating databucket in aws s3\n",
    "2. ingesting raw data with timestamp in the folder raw_data and also downloading locally\n",
    "3. generating log report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f3da2ad-4680-45e5-89b6-6e8dbfb3da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "from datetime import datetime\n",
    "\n",
    "# AWS S3 Configuration\n",
    "S3_BUCKET = \"dmml-bank-churn-data\"\n",
    "S3_FOLDER = \"raw_data/\"\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Sets up logging for raw data storage.\"\"\"\n",
    "    log_file = \"logs/raw_data_storage.log\"\n",
    "    os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "    logging.basicConfig(\n",
    "        filename=log_file,\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    )\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "def upload_to_s3(file_path, s3_key):\n",
    "    \"\"\"Uploads a file to AWS S3 with versioning.\"\"\"\n",
    "    try:\n",
    "        s3.upload_file(file_path, S3_BUCKET, s3_key)\n",
    "        logging.info(f\"Uploaded {file_path} to S3 as {s3_key}.\")\n",
    "    except NoCredentialsError:\n",
    "        logging.error(\"AWS credentials not found.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to upload {file_path} to S3: {e}\")\n",
    "\n",
    "def push_raw_data_to_s3():\n",
    "    \"\"\"Uploads the locally stored raw data to S3 with timestamped versioning.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    raw_data_dir = \"raw_data/\"\n",
    "    if not os.path.exists(raw_data_dir):\n",
    "        logging.error(\"Raw data directory does not exist. Please run data ingestion first.\")\n",
    "        return\n",
    "    \n",
    "    for file in os.listdir(raw_data_dir):\n",
    "        local_file_path = os.path.join(raw_data_dir, file)\n",
    "        s3_key = f\"{S3_FOLDER}{timestamp}_{file}\"\n",
    "        upload_to_s3(local_file_path, s3_key)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to push raw data to S3.\"\"\"\n",
    "    logging.info(\"Starting raw data upload to S3...\")\n",
    "    push_raw_data_to_s3()\n",
    "    logging.info(\"Raw data upload completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    setup_logging()\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71efee41-47f3-426f-8786-10e90b365744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE eda_reports/\n",
      "                           PRE processed_data/\n",
      "                           PRE raw_data/\n",
      "                           PRE reports/\n",
      "                           PRE transformed_data/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File association not found for extension .py\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://dmml-bank-churn-data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b35853f9-48ef-4955-8091-08409ae45b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-08 19:19:47    8196887 2025-03-08_19-19-46_api_data.csv\n",
      "2025-03-08 19:19:48    8196887 2025-03-08_19-19-46_kaggle_data.csv\n",
      "2025-03-08 19:19:49    8086863 2025-03-08_19-19-46_test.csv\n",
      "2025-03-08 19:19:50   12350130 2025-03-08_19-19-46_train.csv\n",
      "2025-03-08 20:20:16    8196887 2025-03-08_20-20-15_api_data.csv\n",
      "2025-03-08 20:20:17    8086863 2025-03-08_20-20-15_kaggle_data.csv\n",
      "2025-03-08 20:20:18   12350130 2025-03-08_20-20-15_train.csv\n",
      "2025-03-08 21:49:24    8196887 2025-03-08_21-49-23_api_data.csv\n",
      "2025-03-08 21:49:25    8086863 2025-03-08_21-49-23_kaggle_data.csv\n",
      "2025-03-08 21:49:26    8086863 2025-03-08_21-49-23_test.csv\n",
      "2025-03-08 21:49:27   12350130 2025-03-08_21-49-23_train.csv\n",
      "2025-03-09 12:19:37    8196887 2025-03-09_12-19-35_api_data.csv\n",
      "2025-03-09 12:19:39    8086863 2025-03-09_12-19-35_kaggle_data.csv\n",
      "2025-03-09 12:19:39    8086863 2025-03-09_12-19-35_test.csv\n",
      "2025-03-09 12:19:40   12350130 2025-03-09_12-19-35_train.csv\n",
      "2025-03-11 11:37:01    8196887 2025-03-11_11-36-59_api_data.csv\n",
      "2025-03-11 11:37:02    8086863 2025-03-11_11-36-59_kaggle_data.csv\n",
      "2025-03-11 11:37:03    8086863 2025-03-11_11-36-59_test.csv\n",
      "2025-03-11 11:37:04   12350130 2025-03-11_11-36-59_train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File association not found for extension .py\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://dmml-bank-churn-data/raw_data/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b34e999-99a3-433c-b45a-fe4dd165fb71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
